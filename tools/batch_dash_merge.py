#!/usr/bin/env python3
"""
æ‰¹é‡DASHåˆå¹¶è„šæœ¬
é«˜æ•ˆå¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶å¤¹ä¸­çš„DASHè§†é¢‘åˆ†æ®µï¼Œæä¾›ç”¨æˆ·å‹å¥½çš„ç•Œé¢å’Œè¯¦ç»†çš„å¤„ç†æŠ¥å‘Š

ç‰¹ç‚¹:
- å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶å¤¹
- å®æ—¶è¿›åº¦æ˜¾ç¤º
- è¯¦ç»†çš„å¤„ç†æŠ¥å‘Š
- é”™è¯¯æ¢å¤å’Œé‡è¯•
- ç”¨æˆ·å‹å¥½çš„è¾“å…¥è¾“å‡ºæœºåˆ¶
"""

import os
import sys
import time
import json
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.combiners.dash_merger import DashMerger


@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    folder_name: str
    input_path: str
    output_path: str
    success: bool
    start_time: float
    end_time: float
    duration: float
    error_message: str = ""
    files_processed: int = 0
    total_size_mb: float = 0.0


class BatchDashMerger:
    """æ‰¹é‡DASHåˆå¹¶å™¨"""
    
    def __init__(self, max_workers: int = 4, verbose: bool = False):
        self.max_workers = max_workers
        self.verbose = verbose
        self.logger = self._setup_logging()
        self.results: List[ProcessingResult] = []
        self.start_time = time.time()
        self.progress_lock = threading.Lock()
        self.completed_count = 0
        self.total_count = 0
        
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            level = logging.DEBUG if self.verbose else logging.INFO
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(level)
        return logger
    
    def scan_dash_folders(self, parent_dir: Path) -> List[Path]:
        """æ‰«æåŒ…å«DASHæ–‡ä»¶çš„æ–‡ä»¶å¤¹"""
        dash_folders = []
        
        if not parent_dir.exists() or not parent_dir.is_dir():
            self.logger.error(f"Parent directory does not exist: {parent_dir}")
            return dash_folders
        
        for folder in parent_dir.iterdir():
            if folder.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«.m4sæ–‡ä»¶
                m4s_files = list(folder.glob("*.m4s"))
                if m4s_files:
                    dash_folders.append(folder)
                    self.logger.debug(f"Found DASH folder: {folder.name} ({len(m4s_files)} m4s files)")
        
        return dash_folders
    
    def get_folder_info(self, folder_path: Path) -> Dict[str, any]:
        """è·å–æ–‡ä»¶å¤¹ä¿¡æ¯"""
        m4s_files = list(folder_path.glob("*.m4s"))
        init_files = list(folder_path.glob("init.mp4"))
        
        total_size = sum(f.stat().st_size for f in m4s_files + init_files)
        total_size_mb = total_size / (1024 * 1024)
        
        # åˆ†æPæ ‡è¯†ç¬¦
        identifiers = set()
        for file in m4s_files:
            merger = DashMerger()
            file_info = merger.parse_m4s_filename(file.name)
            if file_info:
                identifiers.add(file_info['identifier'])
        
        return {
            'total_files': len(m4s_files),
            'init_files': len(init_files),
            'total_size_mb': total_size_mb,
            'identifiers': sorted(identifiers),
            'identifier_count': len(identifiers)
        }
    
    def display_scan_summary(self, dash_folders: List[Path], parent_dir: Path):
        """æ˜¾ç¤ºæ‰«ææ‘˜è¦"""
        print(f"\nğŸ” æ‰«æç›®å½•: {parent_dir}")
        print("=" * 80)
        
        if not dash_folders:
            print("âŒ æœªæ‰¾åˆ°åŒ…å«DASHæ–‡ä»¶çš„æ–‡ä»¶å¤¹")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(dash_folders)} ä¸ªDASHæ–‡ä»¶å¤¹:")
        print("-" * 80)
        
        total_files = 0
        total_size = 0.0
        
        for i, folder in enumerate(dash_folders, 1):
            info = self.get_folder_info(folder)
            total_files += info['total_files']
            total_size += info['total_size_mb']
            
            print(f"{i:3d}. {folder.name}")
            print(f"     ğŸ“„ æ–‡ä»¶: {info['total_files']} m4s + {info['init_files']} init")
            print(f"     ğŸ“Š å¤§å°: {info['total_size_mb']:.1f} MB")
            print(f"     ğŸ¬ æ®µè½: {info['identifier_count']} ä¸ª (P{', P'.join(info['identifiers'])})")
            print()
        
        print("-" * 80)
        print(f"ğŸ“Š æ€»è®¡: {len(dash_folders)} æ–‡ä»¶å¤¹, {total_files} æ–‡ä»¶, {total_size:.1f} MB")
        print("=" * 80)
    
    def process_single_folder(self, folder_path: Path, output_dir: Path) -> ProcessingResult:
        """å¤„ç†å•ä¸ªæ–‡ä»¶å¤¹"""
        start_time = time.time()
        folder_name = folder_path.name
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = output_dir / f"{folder_name}.mp4"
        
        # è·å–æ–‡ä»¶å¤¹ä¿¡æ¯
        folder_info = self.get_folder_info(folder_path)
        
        try:
            # åˆ›å»ºDASHåˆå¹¶å™¨
            merger = DashMerger(verbose=False)  # å‡å°‘æ—¥å¿—è¾“å‡ºé¿å…æ··ä¹±
            
            # æ‰§è¡Œåˆå¹¶
            success = merger.merge_single_folder(folder_path, output_file, dry_run=False)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # æ›´æ–°è¿›åº¦
            with self.progress_lock:
                self.completed_count += 1
                progress = (self.completed_count / self.total_count) * 100
                print(f"âœ… [{self.completed_count}/{self.total_count}] {progress:.1f}% | {folder_name} | {duration:.1f}s")
            
            return ProcessingResult(
                folder_name=folder_name,
                input_path=str(folder_path),
                output_path=str(output_file) if success else "",
                success=success,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                files_processed=folder_info['total_files'],
                total_size_mb=folder_info['total_size_mb']
            )
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            with self.progress_lock:
                self.completed_count += 1
                progress = (self.completed_count / self.total_count) * 100
                print(f"âŒ [{self.completed_count}/{self.total_count}] {progress:.1f}% | {folder_name} | é”™è¯¯: {str(e)}")
            
            return ProcessingResult(
                folder_name=folder_name,
                input_path=str(folder_path),
                output_path="",
                success=False,
                start_time=start_time,
                end_time=end_time,
                duration=duration,
                error_message=str(e),
                files_processed=folder_info['total_files'],
                total_size_mb=folder_info['total_size_mb']
            )
    
    def process_batch(self, parent_dir: Path, output_dir: Path, dry_run: bool = False) -> List[ProcessingResult]:
        """æ‰¹é‡å¤„ç†DASHæ–‡ä»¶å¤¹"""
        # æ‰«ææ–‡ä»¶å¤¹
        dash_folders = self.scan_dash_folders(parent_dir)
        
        if not dash_folders:
            return []
        
        # æ˜¾ç¤ºæ‰«ææ‘˜è¦
        self.display_scan_summary(dash_folders, parent_dir)
        
        if dry_run:
            print("\nğŸ” æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…å¤„ç†æ–‡ä»¶")
            return []
        
        # ç¡®è®¤å¤„ç†
        try:
            response = input(f"\nğŸ¤” ç¡®å®šè¦å¤„ç†è¿™ {len(dash_folders)} ä¸ªæ–‡ä»¶å¤¹å—? (y/N): ").strip().lower()
            if response not in ['y', 'yes']:
                print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return []
        except KeyboardInterrupt:
            print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            return []
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        self.total_count = len(dash_folders)
        self.completed_count = 0
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† ({self.max_workers} ä¸ªå¹¶è¡Œä»»åŠ¡)")
        print("=" * 80)
        
        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_folder = {
                executor.submit(self.process_single_folder, folder, output_dir): folder 
                for folder in dash_folders
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_folder):
                result = future.result()
                self.results.append(result)
        
        return self.results
    
    def generate_report(self, output_dir: Path) -> Path:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        if not self.results:
            return None
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_time = time.time() - self.start_time
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        total_files = sum(r.files_processed for r in self.results)
        total_size = sum(r.total_size_mb for r in self.results)
        processing_time = sum(r.duration for r in self.results)
        
        # åˆ›å»ºæŠ¥å‘Š
        report = {
            'summary': {
                'total_folders': len(self.results),
                'successful': len(successful),
                'failed': len(failed),
                'success_rate': len(successful) / len(self.results) * 100,
                'total_time_seconds': total_time,
                'total_processing_time_seconds': processing_time,
                'total_files_processed': total_files,
                'total_size_mb': total_size,
                'average_speed_mb_per_second': total_size / processing_time if processing_time > 0 else 0,
                'parallel_efficiency': processing_time / total_time if total_time > 0 else 0,
                'timestamp': datetime.now().isoformat()
            },
            'successful_folders': [asdict(r) for r in successful],
            'failed_folders': [asdict(r) for r in failed]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = output_dir / f"batch_dash_merge_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report_file
    
    def display_final_summary(self, report_file: Optional[Path] = None):
        """æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦"""
        if not self.results:
            return
        
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        total_time = time.time() - self.start_time
        total_files = sum(r.files_processed for r in self.results)
        total_size = sum(r.total_size_mb for r in self.results)
        
        print("\n" + "=" * 80)
        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆæ‘˜è¦")
        print("=" * 80)
        
        print(f"âœ… æˆåŠŸ: {len(successful)} ä¸ªæ–‡ä»¶å¤¹")
        print(f"âŒ å¤±è´¥: {len(failed)} ä¸ªæ–‡ä»¶å¤¹")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {len(successful)/len(self.results)*100:.1f}%")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {total_files} ä¸ª")
        print(f"ğŸ“Š å¤„ç†å¤§å°: {total_size:.1f} MB")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_size/total_time:.1f} MB/s")
        
        if failed:
            print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶å¤¹:")
            for result in failed:
                print(f"   â€¢ {result.folder_name}: {result.error_message}")
        
        if report_file:
            print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡DASHåˆå¹¶å·¥å…· - é«˜æ•ˆå¹¶è¡Œå¤„ç†å¤šä¸ªDASHæ–‡ä»¶å¤¹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python tools/batch_dash_merge.py C:\\Users\\carll\\Desktop\\catDownloads
  
  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œå¹¶è¡Œæ•°
  python tools/batch_dash_merge.py C:\\Users\\carll\\Desktop\\catDownloads -o D:\\Output -w 6
  
  # æ¨¡æ‹Ÿè¿è¡Œï¼ˆä»…æ‰«æï¼Œä¸å¤„ç†ï¼‰
  python tools/batch_dash_merge.py C:\\Users\\carll\\Desktop\\catDownloads --dry-run
  
  # è¯¦ç»†è¾“å‡ºæ¨¡å¼
  python tools/batch_dash_merge.py C:\\Users\\carll\\Desktop\\catDownloads --verbose
        """
    )
    
    parser.add_argument('input_dir', type=Path, help='åŒ…å«DASHæ–‡ä»¶å¤¹çš„çˆ¶ç›®å½•')
    parser.add_argument('-o', '--output-dir', type=Path, help='è¾“å‡ºç›®å½• (é»˜è®¤ä¸ºè¾“å…¥ç›®å½•ä¸‹çš„mergedæ–‡ä»¶å¤¹)')
    parser.add_argument('-w', '--workers', type=int, default=4, help='å¹¶è¡Œå¤„ç†ä»»åŠ¡æ•° (é»˜è®¤: 4)')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä»…æ‰«æä¸å¤„ç†')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥ç›®å½•
    if not args.input_dir.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return 1
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = args.output_dir or (args.input_dir / "merged")
    
    print(f"ğŸ¬ VREconder æ‰¹é‡DASHåˆå¹¶å·¥å…·")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”§ å¹¶è¡Œä»»åŠ¡: {args.workers}")
    
    try:
        # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
        batch_merger = BatchDashMerger(max_workers=args.workers, verbose=args.verbose)
        
        # æ‰§è¡Œæ‰¹é‡å¤„ç†
        results = batch_merger.process_batch(args.input_dir, output_dir, dry_run=args.dry_run)
        
        if results and not args.dry_run:
            # ç”ŸæˆæŠ¥å‘Š
            report_file = batch_merger.generate_report(output_dir)
            
            # æ˜¾ç¤ºæ‘˜è¦
            batch_merger.display_final_summary(report_file)
        
        # è¿”å›é€‚å½“çš„é€€å‡ºä»£ç 
        if not results:
            return 0 if args.dry_run else 1
        
        failed_count = sum(1 for r in results if not r.success)
        return 0 if failed_count == 0 else 1
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 130
    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 